{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5dbf4d",
   "metadata": {},
   "source": [
    "# AI dialogue exchange\n",
    "\n",
    "A playful yet powerful AI experiment, where two local language models engage in an ongoing conversation, responding to each other’s musings, arguments, and quips. This project gives you a front-row seat to an AI debate club that never sleeps.\n",
    "\n",
    "AI dialogue exchange isn’t just a tech demo—it’s a mini stage for observing AI behavior, testing inference performance, and exploring how LLMs respond when left to their own devices—or to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d40595",
   "metadata": {},
   "source": [
    "## 1 Install and verify packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beba691",
   "metadata": {},
   "source": [
    "### 1.1 Python (>= 3.8)\n",
    "\n",
    "Check Python is installed and its version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460aebd6",
   "metadata": {},
   "source": [
    "### 1.2 Install Ollama Python client\n",
    "\n",
    "Install required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85808a",
   "metadata": {},
   "source": [
    "### 1.3 Ollama client\n",
    "\n",
    "Check Ollama is installed and make sure required models are pulled locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b4741",
   "metadata": {},
   "source": [
    "## 2 Setup\n",
    "\n",
    "Setup code for experement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63af8a",
   "metadata": {},
   "source": [
    "### 2.1 Imports and constants\n",
    "\n",
    "Python imports and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09461f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import textwrap\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "MESSAGE_HISTORY = [] # History of messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b3fc1",
   "metadata": {},
   "source": [
    "### 2.2 Set system prompt\n",
    "\n",
    "System prompt, change this to tweak how the AI responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_prompt():\n",
    "    return \"You are an AI engaging in a thoughtful, casual, and respectful conversation with another AI.\" \\\n",
    "    \" Speak in a friendly and clear tone—avoid being too formal or robotic.\" \\\n",
    "    \" Don’t just ask questions back and forth.\" \\\n",
    "    \" Share your insights, perspectives, or examples related to the topic.\" \\\n",
    "    \" If you don't know something, say so briefly, and pivot back to the topic.\" \\\n",
    "    \" Avoid saying things like “Thank you” or “Glad I could help”—focus on adding value to the discussion.\" \\\n",
    "    \" Always keep the conversation alive by contributing new ideas, angles, or points worth exploring.\" \\\n",
    "    \" Keep all responses under 150 words.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72166701",
   "metadata": {},
   "source": [
    "### 2.3 Initialize constants\n",
    "\n",
    "Provide details about your AI models here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de338bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_AI = {\"model\": \"llama3.2\", \"title\": \"Llama\"}\n",
    "GEMMA_AI = {\"model\": \"gemma\", \"title\": \"Gemma\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449f251f",
   "metadata": {},
   "source": [
    "### 2.4 Select AI models\n",
    "\n",
    "Select the AI models for conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_USER = {\"model\": \"ACTUAL_USER\",  \"title\": \"User\", \"color\": \"#1E90FF\", \"code\": \"User\"} # Don't update this.\n",
    "# Set agents to use\n",
    "AI_01 = GEMMA_AI | {\"code\": \"AI_01\", \"color\": \"#DAA520\"}\n",
    "AI_02 = LLAMA_AI | {\"code\": \"AI_02\", \"color\": \"#3CB371\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f1e29",
   "metadata": {},
   "source": [
    "### 2.5 Seed topic of discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b857a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default seed topic of discussion\n",
    "SEED_TOPIC = \"What is meaning of life?\"\n",
    "# Other interesting topics\n",
    "# SEED_TOPIC = \"What is the best way for LLMs to collaborate on tasks?\"\n",
    "# SEED_TOPIC = \"Can LLMs develop unique styles or 'personalities'?\"\n",
    "# SEED_TOPIC = \"What strategies help LLMs stay on-topic and relevant?\"\n",
    "# SEED_TOPIC = \"What would an AI’s concept of 'truth' look like?\"\n",
    "# SEED_TOPIC = \"Are LLMs creative, or just remixers of human creativity?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9348f8",
   "metadata": {},
   "source": [
    "### 2.6 Define Ollama chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_ollama_model(ai_details, messages):\n",
    "    model = ai_details[\"model\"]\n",
    "\n",
    "    response = ollama.chat(model=model, messages=messages)\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12901e0",
   "metadata": {},
   "source": [
    "### 2.7 Ollama message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ollama_messages(ai_details):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": get_system_prompt()})\n",
    "    for msg in MESSAGE_HISTORY:\n",
    "        msg_ai_details = msg[\"ai_details\"]\n",
    "        msg_content = msg[\"content\"]\n",
    "        if msg_ai_details[\"code\"] == ai_details[\"code\"]:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": msg_content})\n",
    "        else:\n",
    "            messages.append({\"role\": \"user\", \"content\": msg_content})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af837717",
   "metadata": {},
   "source": [
    "### 2.8 get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(ai_details):\n",
    "    # print(\"get_response - gradio_history: \", gradio_history)\n",
    "    messages = build_ollama_messages(ai_details)\n",
    "    # print(\"get_response - messages: \", messages)\n",
    "    response = chat_with_ollama_model(ai_details, messages)\n",
    "    # print(\"get_response - response: \", response)\n",
    "    MESSAGE_HISTORY.append({\"ai_details\": ai_details, \"content\": response})\n",
    "    return MESSAGE_HISTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541217ec",
   "metadata": {},
   "source": [
    "## 3 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639f6be",
   "metadata": {},
   "source": [
    "### 3.1 First message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MESSAGE_HISTORY = []\n",
    "MESSAGE_HISTORY.append({\"ai_details\": ACTUAL_USER, \"content\": SEED_TOPIC})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb4b45",
   "metadata": {},
   "source": [
    "### 3.2 Test AI - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = get_response(AI_01)\n",
    "for h in history:\n",
    "    print(h[\"ai_details\"][\"title\"], \": \", h[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920315eb",
   "metadata": {},
   "source": [
    "### 3.3 Test AI - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(AI_02)\n",
    "for h in history:\n",
    "    print(h[\"ai_details\"][\"title\"], \": \", h[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709700f9",
   "metadata": {},
   "source": [
    "## 4. Showdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59561319",
   "metadata": {},
   "source": [
    "### 4.1 Clean history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_messages():\n",
    "    global MESSAGE_HISTORY\n",
    "    MESSAGE_HISTORY = []\n",
    "    MESSAGE_HISTORY.append({\"ai_details\": ACTUAL_USER, \"content\": SEED_TOPIC})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ed3b3",
   "metadata": {},
   "source": [
    "### 4.2 Time for play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93327f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHATS = 10\n",
    "WRAP_WIDTH = 200\n",
    "\n",
    "clean_messages()\n",
    "\n",
    "def print_message():\n",
    "    h = MESSAGE_HISTORY[-1]\n",
    "    color = h[\"ai_details\"][\"color\"]\n",
    "    title = h[\"ai_details\"][\"code\"] + \"/\" + h[\"ai_details\"][\"title\"]\n",
    "    msg = \"<span style='color:\" + color + \";'><b>\" + title + \"</b>: \" + h[\"content\"] + \"</span>\"\n",
    "    wrapped = textwrap.fill(msg, width=WRAP_WIDTH)\n",
    "    display(HTML(wrapped.replace('\\n', '<br>')))\n",
    "\n",
    "print_message()\n",
    "\n",
    "for i in range(MAX_CHATS):\n",
    "    if i % 2 == 0:\n",
    "        get_response(AI_01)\n",
    "    else:\n",
    "        get_response(AI_02)\n",
    "    print_message()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
